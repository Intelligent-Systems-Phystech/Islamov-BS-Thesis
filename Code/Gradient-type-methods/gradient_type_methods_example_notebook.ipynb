{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli\n",
    "from oracles import LogReg\n",
    "from methods import Standard_Newton,\\\n",
    "Basic_method, PositiveCase_method, GeneralCase_method\n",
    "from utils import function_plot_builder, bits_plot_builder, bits_plot_builder1\n",
    "from methods import CubicMaxNewton, CubicMaxNewtonP\n",
    "from methods import DINGO, GeneralCase_methodP, PositiveCase_methodP\n",
    "from easydict import EasyDict\n",
    "from methods import diana, adiana, dcgd\n",
    "from utils import loss_logistic, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './Datasets/a2a.txt'\n",
    "data_name = 'a2a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization parameter\n",
    "lmb = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes, size of local data, and number of weights\n",
    "N = 2265    \n",
    "n = 15         \n",
    "m = 151             \n",
    "d = 123 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data reading\n",
    "b = np.zeros((N,))   \n",
    "A = np.zeros((N, d))\n",
    "\n",
    "f = open(dataset_path, 'r')\n",
    "for i, line in enumerate(f):\n",
    "    if i < N:\n",
    "        line = line.split()\n",
    "        for c in line:\n",
    "            if c == '+1':\n",
    "                b[i] = 1\n",
    "            elif c == '-1':\n",
    "                b[i] = -1\n",
    "            elif c == '\\n':\n",
    "                continue\n",
    "            else:\n",
    "                c = c.split(':')\n",
    "                A[i][int(c[0]) - 1] = float(c[1]) \n",
    "                \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logistic regression problem\n",
    "logreg = LogReg(A=A, b=b, reg_coef=lmb, n=n, m=m, d=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal solution using Newton's method\n",
    "SN = Standard_Newton(logreg)\n",
    "SN.find_optimum(np.zeros(d), n_steps=20)\n",
    "x_opt = logreg.get_optimum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shift\n",
    "shift = np.ones(d)*0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient type methods\n",
    "\n",
    "Note that we implemented gradient type methods which return total number of bits for one node. It means that if you want to obtain the total number of bits sent by all nodes you should multiply it by\n",
    "\n",
    "- $2n$ for ADIANA\n",
    "- $n$ for DIANA\n",
    "- $n$ for DCGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(EasyDict):\n",
    "    def __init__(self, data_name, T, node, L, lamda, eta=0.05, alpha=0.5, theta_1=0.25, theta_2=0.5, gamma=0.5,\n",
    "                 beta=0.95,\n",
    "                 prob=1,\n",
    "                 comp_method='no_comp',\n",
    "                 r=None, s=None, plotn=100, ID=1, s_level=10):\n",
    "        super().__init__()\n",
    "        self.ID = ID\n",
    "        self.data_name = data_name\n",
    "        self.T = T\n",
    "        self.plotn = plotn\n",
    "        self.node = node\n",
    "        self.eta = eta\n",
    "        self.L = L\n",
    "        self.lamda = lamda\n",
    "        self.alpha = alpha\n",
    "        self.theta_1 = theta_1\n",
    "        self.theta_2 = theta_2\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.prob = prob\n",
    "        self.comp_method = comp_method\n",
    "        self.r = r\n",
    "        self.s = s\n",
    "        self.s_level = s_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADIANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find estimation of L\n",
    "\n",
    "H = np.dot(A.T,A)/N\n",
    "temp = np.linalg.eigvalsh(H)\n",
    "L = np.abs(temp[-1])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of methods\n",
    "\n",
    "max_iter = 10000 \n",
    "arg = args(data_name, max_iter, n, L, lmb)\n",
    "arg.r = d/4\n",
    "arg.s = np.sqrt(d)\n",
    "arg.plotn = 200\n",
    "comp_methods = [\n",
    "     'rand_sparse'\n",
    "]\n",
    "arg.comp_method = comp_methods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_opt + shift\n",
    "loss_adiana, com_bits_adiana = adiana(A, b, x, arg, f_opt=loss_logistic(A, b, x_opt, arg), tol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DIANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find estimation of L\n",
    "\n",
    "H = np.dot(A.T,A)/N\n",
    "temp = np.linalg.eigvalsh(H)\n",
    "L = np.abs(temp[-1])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of methods\n",
    "\n",
    "max_iter = 10 \n",
    "arg = args(data_name, max_iter, n, L, lmb)\n",
    "arg.r = d/4\n",
    "arg.s = np.sqrt(d)\n",
    "arg.plotn = 200\n",
    "comp_methods = [\n",
    "     'natural_comp'\n",
    "]\n",
    "arg.comp_method = comp_methods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_opt + shift\n",
    "loss_diana, com_bits_diana = diana(A, b, x, arg, f_opt=loss_logistic(A, b, x_opt, arg), tol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DCGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find estimation of L\n",
    "\n",
    "H = np.dot(A.T,A)/N\n",
    "temp = np.linalg.eigvalsh(H)\n",
    "L = np.abs(temp[-1])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of methods\n",
    "\n",
    "max_iter = 10 \n",
    "arg = args(data_name, max_iter, n, L, lmb)\n",
    "arg.r = d/4\n",
    "arg.s = np.sqrt(d)\n",
    "arg.plotn = 200\n",
    "comp_methods = [\n",
    "     'rand_dithering'\n",
    "]\n",
    "arg.comp_method = comp_methods[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_opt + shift\n",
    "loss_dcgd, com_bits_dcgd = dcgd(A, b, x, arg, f_opt=loss_logistic(A, b, x_opt, arg), tol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
